{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, current_timestamp, window\n\nspark = (SparkSession.builder\n    .appName(\"Kafka Structured Streaming\")\n    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4\")\n    .master(\"local[*]\")\n    .getOrCreate())\n\nspark.sparkContext.setLogLevel(\"WARN\")\n\nkafka_df = (spark.readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\",\"localhost:9092\")\n    .option(\"subscribe\",\"word_topic\")\n    .load())\n\nlines = kafka_df.selectExpr(\"CAST(value AS STRING)\")\nwords = (lines.withColumn(\"timestamp\", current_timestamp())\n    .select(explode(split(lines.value,\" \")).alias(\"word\"), \"timestamp\"))\n\ncounts = (words\n    .withWatermark(\"timestamp\",\"10 seconds\")\n    .groupBy(window(\"timestamp\",\"1 minute\"), \"word\")\n    .count())\n\nquery = (counts.writeStream\n    .outputMode(\"append\")\n    .format(\"parquet\")\n    .option(\"checkpointLocation\",\"checkpoints\")\n    .option(\"path\",\"output\")\n    .start())\n\nquery.awaitTermination(300)\nspark.stop()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}