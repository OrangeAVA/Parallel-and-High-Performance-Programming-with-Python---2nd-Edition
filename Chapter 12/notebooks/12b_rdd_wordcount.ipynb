{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"RDD WordCount\").master(\"local[*]\").getOrCreate()\n\nlines = [\"PySpark is powerful and fast\",\n         \"Distributed computing is useful for large datasets\",\n         \"PySpark supports parallel computing\",\n         \"with Spark we can process large volumes of data\"]\n\nrdd = spark.sparkContext.parallelize(lines)\n\nwords = rdd.flatMap(lambda line: line.split(\" \"))\nword_pairs = words.map(lambda w: (w.lower(), 1))\nword_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n\ndf_word_counts = word_counts.toDF([\"Word\", \"Count\"]).orderBy(\"Count\", ascending=False)\ndf_word_counts.show(truncate=False)\n\nspark.stop()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}